{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c0d420",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datatable as dt\n",
    "import gc\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set_color_codes(\"pastel\") # 颜色设定\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['pdf.fonttype'] = 42\n",
    "sns.set_color_codes(\"muted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4af2154",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LoadTables(filename, sepstr, usecols):\n",
    "    dt_df = dt.fread(filename, sep=sepstr, header=True, columns=usecols, fill=True)\n",
    "    df  = dt_df.to_pandas()\n",
    "    del dt_df\n",
    "    gc.collect()\n",
    "    return(df)\n",
    "\n",
    "# Filter\n",
    "def FilterDF(VDF_DF, region, filter_Frag):\n",
    "    ## region filter\n",
    "    print(\"Befor filter : %d\"%len(VDF_DF))\n",
    "    Pchr = VDF_DF.chrom == region[0]\n",
    "    Pregion = (VDF_DF.start >= region[1]) & (VDF_DF.end <= region[2])\n",
    "    VDF_filter = VDF_DF.loc[ Pchr & Pregion , :]\n",
    "    print(\"After Region Filter: %d\"%len(VDF_filter) )\n",
    "    ## fragment filter\n",
    "    Fragmentcount = VDF_filter.groupby(by=\"read_name\", as_index=True)[\"chrom\"].count()\n",
    "    VDF_filter = VDF_filter.set_index(\"read_name\")\n",
    "    VDF_filter[\"Fragnum\"] = 0\n",
    "    VDF_filter.loc[:, \"Fragnum\"] = Fragmentcount.loc[VDF_filter.index]\n",
    "    VDF_filter = VDF_filter.loc[VDF_filter.Fragnum>=filter_Frag, :] \n",
    "    print(\"After Fragment number Filter: %d\"%len(VDF_filter) )\n",
    "    return (VDF_filter)\n",
    "\n",
    "# Bins \n",
    "def BinsDF(df, binsize=5000):\n",
    "    df = df.reset_index()\n",
    "    df[\"pos\"] = (df.start.values + df.end.values)/2\n",
    "    df[\"pos\"] = df[\"pos\"].astype(\"int\")\n",
    "    df[\"bin\"] =  ( df[\"pos\"].values/binsize ).astype(\"int\")\n",
    "    #df = df.drop([\"start\", \"end\", \"Fragnum\"], axis=1)\n",
    "    return (df)\n",
    "\n",
    "# Loading\n",
    "def Loading(filename, region, filter_Frag, binsize = 5000):\n",
    "    print(\"Loading %s\"%filename)\n",
    "    usecols = {\"read_name\",\"read_start\",\"read_end\",\"strand\",\"chrom\",\"start\",\"end\",\"MapQual\",\"LRvdF_pfix\"}\n",
    "    VDF_DF = LoadTables(filename, \",\",  usecols)\n",
    "    VDF_DF.columns = [\"read_name\",\"read_start\",\"read_end\",\"strand\",\"chrom\",\"start\",\"end\",\"MapQual\",\"LRvdF_pfix\"]\n",
    "    VDF_filter = FilterDF(VDF_DF, region, filter_Frag)\n",
    "    del(VDF_DF)\n",
    "    gc.collect()\n",
    "    # Bin calculate\n",
    "    bin_df = BinsDF(VDF_filter, binsize)\n",
    "    del(VDF_filter)\n",
    "    return(bin_df)\n",
    "\n",
    "# Loading bed\n",
    "def Loadingbed(filename, region, binsize, colnames = [\"chrom\", \"start\", \"end\", \"value\"], usecols = [0,1,2,6] ):\n",
    "    print(\"Loading %s\"%filename)\n",
    "    sepstr = \"\\t\"\n",
    "    bedDF = pd.read_table(filename, sep=sepstr, header=None, usecols=usecols)\n",
    "    bedDF.columns = colnames\n",
    "    Pchr = bedDF.chrom == region[0]\n",
    "    Pregion = (bedDF.start >= region[1]) & (bedDF.end <= region[2])\n",
    "    bedDF = bedDF.loc[Pchr&Pregion,:]\n",
    "    bedDF = BinsDF(bedDF, binsize)\n",
    "    return(bedDF)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07f8ef4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Loading datas\n",
    "# Loading read fragment annotation and filter\n",
    "# Filter \n",
    "filter_Frag =  2\n",
    "binsize = 5000\n",
    "## region\n",
    "region = [\"chr2\", 121340000,121810000]\n",
    "filename = \"chr2_reads_Align_Fragment_RvdF.csv\"\n",
    "binVDF = Loading(filename, region, filter_Frag, binsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76162ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading CTCF bed\n",
    "ctcffile=\"ENCFF796WRU_GM12878_CTCF_narrow_peaks.bed\"\n",
    "bedDF = Loadingbed(ctcffile, region, binsize, [\"chrom\", \"start\", \"end\", \"value\"] )\n",
    "bedDF = bedDF.set_index(\"bin\")\n",
    "bedDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b684d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ctcf bin match\n",
    "ctcfbins = set( bedDF.index.values )\n",
    "binVDF[\"binfind\"] = 0\n",
    "binVDF.loc[ binVDF.bin.isin( ctcfbins ),\"binfind\"] = 1\n",
    "binVDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb74d4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bin Matrix\n",
    "binsize=5000\n",
    "readnames = list( set( binVDF.read_name.to_list() ) )\n",
    "Ts, Te = round(region[1]/binsize), round( region[2]/binsize + 1 )\n",
    "BinIds = [ idx for idx in range( Ts, Te ) ]\n",
    "\n",
    "FragMatrix = pd.DataFrame( np.zeros( [len(BinIds), len(readnames)], dtype=int ), \n",
    "                          index = BinIds,\n",
    "                          columns = readnames)\n",
    "binVDF_group = binVDF.groupby(\"read_name\")\n",
    "for read_name, df in binVDF_group:\n",
    "    P = (df.bin>= Ts) & (df.bin<Te)\n",
    "    FragMatrix.loc[ df.loc[P,\"bin\"].values, read_name ] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8bcb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster region fragments >= 3\n",
    "FragSelect=\"mdLRMFs\"\n",
    "if FragSelect == \"All\":\n",
    "    c_frag_num = 2\n",
    "    cluster_P = FragMatrix.sum() >= c_frag_num\n",
    "elif FragSelect == \"lwLRMFs\":\n",
    "    c_frag_num = 2\n",
    "    cluster_P = FragMatrix.sum() <= c_frag_num\n",
    "elif FragSelect == \"mdLRMFs\":\n",
    "    cfnl, cfnh = 3, 50\n",
    "    cluster_P = ( FragMatrix.sum() <= cfnh ) & ( FragMatrix.sum() >= cfnl )\n",
    "elif FragSelect == \"hgLRMFs\":\n",
    "    cfnl = 7\n",
    "    cluster_P =  FragMatrix.sum() >= cfnl \n",
    "    \n",
    "cluster_reads = cluster_P.loc[cluster_P==True].index.to_list()\n",
    "FragMatrix_filter = FragMatrix.loc[:, cluster_reads]\n",
    "print( \"%d %s reads in %d all reads in cluster region.\" %(len(cluster_reads), FragSelect, len(readnames)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9142806",
   "metadata": {},
   "outputs": [],
   "source": [
    "FragMatrix_filter.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa514f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## hierarchy cluster\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "import scipy.cluster.hierarchy as sch\n",
    "\n",
    "# Cluster\n",
    "clusterDF = sns.clustermap(data=FragMatrix_filter.T, metric=\"euclidean\", method=\"ward\", figsize=(10,6), col_cluster=False)\n",
    "# export\n",
    "## cluster branch tree order\n",
    "dist_thred = 30\n",
    "Z = clusterDF.dendrogram_row.calculated_linkage\n",
    "T = sch.fcluster(Z, t=dist_thred, criterion='distance')\n",
    "print(\"The number of clusters: %d\"%max(T) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46cec0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "## cluster branch tree order\n",
    "dist_thred = 30\n",
    "Z = clusterDF.dendrogram_row.calculated_linkage\n",
    "T = sch.fcluster(Z, t=dist_thred, criterion='distance')\n",
    "print(\"The number of clusters: %d\"%max(T) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95b53cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## cluster leaves order index\n",
    "reorder_idx = clusterDF.dendrogram_row.reordered_ind\n",
    "read_names_order = FragMatrix_filter.iloc[:, reorder_idx].columns.to_list()\n",
    "## tree branch and leaves diction\n",
    "Tdic = {}\n",
    "#T = list(T)[::-1]\n",
    "for i in range( len(T ) ):\n",
    "    Tdic[ FragMatrix_filter.columns[i] ] = T[i]\n",
    "    \n",
    "# hierarchy cluster_reads \n",
    "order_DF = binVDF.set_index(\"read_name\", drop=False).loc[read_names_order]\n",
    "order_DF[\"Cluster\"] = order_DF[\"read_name\"].apply(lambda x: Tdic[x]) \n",
    "readorder = dict()\n",
    "readidx = 0\n",
    "for rn in read_names_order:\n",
    "    readorder[rn] = readidx\n",
    "    readidx += 1 \n",
    "order_DF[\"read_order\"] =  order_DF[\"read_name\"].apply(lambda x: readorder[x] )\n",
    "order_DF = order_DF.reset_index(drop=True)\n",
    "# start end\n",
    "order_DF.loc[:, \"start\"] = (order_DF.bin.values*binsize - 0.5*binsize).astype(int)\n",
    "order_DF.loc[:, \"end\"] = (order_DF.bin.values*binsize + 0.5*binsize).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfde150",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import unique\n",
    "#from sklearn.mixture import GaussianMixture\n",
    "#X = np.array( FragMatrix_filter.T )\n",
    "#n_clusters_ = 3\n",
    "#model = GaussianMixture(n_components=n_clusters_, random_state=10)\n",
    "#model.fit(X)\n",
    "#T  = model.predict(X) # labels\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "tsne = TSNE(n_components=2, random_state=0)\n",
    "X_2d = tsne.fit_transform( FragMatrix_filter.T ) \n",
    "plt.figure(figsize=(6, 5))\n",
    "colors = [\"k\",\"g\", \"b\", \"orange\", \"c\", \"purple\",  \"y\", \"navy\"]\n",
    "target_ids = range(1, T.max() + 1 )\n",
    "\n",
    "legendlist = []\n",
    "for i, c, label in zip(target_ids, colors[1:], target_ids):\n",
    "    plt.scatter(X_2d[T == i, 0], X_2d[T == i, 1], c=c, label=label, alpha=0.5)\n",
    "    legendlist.append(\"%d : %d\"%(i, sum(T==i) ) )\n",
    "plt.legend(legendlist)\n",
    "plt.xlabel(\"tsne1\")\n",
    "plt.ylabel(\"tsne2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d050b7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "legendlist = []\n",
    "plt.figure(figsize=(3, 10))\n",
    "for i, c, label in zip(target_ids, colors[1:], target_ids):\n",
    "    plt.subplot(3,1, i)\n",
    "    plt.scatter(X_2d[T == i, 0], X_2d[T == i, 1], c=c, label=label, alpha=0.5)\n",
    "    legendlist.append(\"%d : %d\"%(i, sum(T==i) ) )\n",
    "    plt.xlim([-100,100])\n",
    "    plt.ylim([-100,100])\n",
    "    plt.legend(legendlist[-1])\n",
    "plt.xlabel(\"tsne1\")\n",
    "plt.ylabel(\"tsne2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a09227e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reads Cluster Figure\n",
    "# 统计Cluster reads的数量\n",
    "def ClusterReads(clusterDF):\n",
    "    groupDF = clusterDF.groupby(\"Cluster\")\n",
    "    \n",
    "    cluster_list, readcount_list = [], []\n",
    "    sumreads = 0\n",
    "    readcount = 0\n",
    "    for cluster, gdF in groupDF:\n",
    "        cluster_list.append(cluster)\n",
    "        readcount = len( list(set(gdF[\"read_name\"].to_list()) ) )\n",
    "        readcount_list.append(readcount)\n",
    "        sumreads += readcount\n",
    "    ratio_list = [ float(c)/sumreads for c in readcount_list ]\n",
    "    cluster_readcount_df = pd.DataFrame({\"Cluster\":cluster_list,\n",
    "                                    \"readcount\":readcount_list,\n",
    "                                    \"ratio\":ratio_list})\n",
    "    return(cluster_readcount_df) \n",
    " \n",
    "# 统计Cluster Fragment 数量占比 分母是总reads 数 或者是 cluster分群reads数\n",
    "def FragmentDensity(clusterDF, Total_reads_num):\n",
    "    # cluster read counts\n",
    "    cluster_readcount_df  = ClusterReads(clusterDF)\n",
    "    bincount_df = clusterDF.groupby([\"Cluster\", \"bin\"], as_index=False)[\"read_name\"].count()\n",
    "    bincount_df.columns = [\"Cluster\", \"bin\", \"count\"]\n",
    "    bincount_df[\"ratio\"] = 0.0\n",
    "    allbins = list( range( bincount_df[\"bin\"].min(),  bincount_df[\"bin\"].max()+1 ) )\n",
    "    ## bin ratio = bincount / Total_reads_num\n",
    "    for i, rcount_row in cluster_readcount_df.iterrows():\n",
    "        cluster_id,  readcount = rcount_row[\"Cluster\"], rcount_row[\"readcount\"]\n",
    "        bincount_df.loc[bincount_df.Cluster==cluster_id, \"ratio\"] = bincount_df.loc[bincount_df.Cluster==cluster_id, \"count\"].values / Total_reads_num\n",
    "        # bin without fragments\n",
    "        bins = bincount_df.loc[bincount_df.Cluster==cluster_id, \"bin\"].to_list()\n",
    "        sbins = [ b for b in allbins if b not in bins ]\n",
    "        sbin_df = pd.DataFrame({\"Cluster\":len(sbins)*[int(cluster_id)],\n",
    "                                \"bin\": sbins,\n",
    "                               \"count\":0,\n",
    "                               \"ratio\":0.0})\n",
    "        bincount_df = pd.concat([bincount_df, sbin_df])\n",
    "    bincount_df = bincount_df.sort_values(by=[\"Cluster\", \"bin\"], ignore_index=True)\n",
    "    bincount_df = bincount_df.astype({\"Cluster\":int, \"bin\":int})\n",
    "    # relative ratio  group total reads\n",
    "    bincount_df.loc[:, \"relative_ratio\"] = 0.0\n",
    "    for Cluster, gdf in bincount_df.groupby(\"Cluster\"):\n",
    "        g_totalreads = Cluster_reads_count.loc[Cluster_reads_count.Cluster==Cluster, \"Count\"][Cluster]\n",
    "        P = bincount_df.Cluster == Cluster\n",
    "        bincount_df.loc[P, \"relative_ratio\"] = bincount_df.loc[P, \"count\"].values / g_totalreads\n",
    "    \n",
    "    return(bincount_df)\n",
    "\n",
    "from scipy.interpolate import make_interp_spline\n",
    "def smoothline(x_array, y_array, smbins=100):\n",
    "    '''\n",
    "    smooth lines\n",
    "    input: x,y array\n",
    "    output: x, smooth_y array\n",
    "    bigger smbins makes  \n",
    "    '''\n",
    "    xi = np.linspace(x_array.min(),x_array.max(),smbins)\n",
    "    y_smooth =  make_interp_spline(x_array, y_array)(xi)\n",
    "    return(xi, y_smooth)\n",
    "\n",
    "read_order_df = order_DF.groupby([\"Cluster\",\"read_name\"])[\"start\"].min().reset_index()\n",
    "read_order_df = read_order_df.sort_values(by=[\"Cluster\",\"start\"], ignore_index=True).reset_index().set_index(\"read_name\")\n",
    "order_DF.loc[:, \"FragY\"] = read_order_df.loc[order_DF.read_name.to_list(), \"index\"].values\n",
    "colors = [\"k\",\"g\", \"b\", \"orange\", \"c\", \"purple\",  \"y\", \"navy\"]\n",
    "subsample = 2 # set subsample reads\n",
    "plt.figure(figsize=(6,6))\n",
    "# Cluster Figure 1\n",
    "ax1 = plt.subplot(5,1,(1,2), rasterized=True ) \n",
    "for cluster, gdf in order_DF.groupby([\"Cluster\", \"read_name\"]):\n",
    "    yn = gdf[\"FragY\"].values[0]\n",
    "    if yn % subsample == 0:\n",
    "        # grey line\n",
    "        read_xs = [ gdf[\"bin\"].min()*binsize, gdf[\"bin\"].max()*binsize]\n",
    "        ys = [yn, yn ]\n",
    "        #plt.plot(read_xs, ys, c=\"lightgrey\", linewidth=0.2) link lines\n",
    "        # Fragments\n",
    "        for indx, rowvalue in gdf.iterrows():\n",
    "            yn = rowvalue[\"FragY\"]\n",
    "            sc = colors[cluster[0]]\n",
    "            #if rowvalue[\"binfind\"] == 1:\n",
    "            #    sc = \"k\" # ctct bin fragments black\n",
    "            start = rowvalue[\"bin\"]*binsize - 0.5*binsize\n",
    "            end = rowvalue[\"bin\"]*binsize + 0.5*binsize\n",
    "            xs = [ start, end ]\n",
    "            ys = [yn, yn ]\n",
    "            # Fragment\n",
    "            plt.plot(xs, ys, c=sc, linewidth=0.5)\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "\n",
    "## Cluster reads density\n",
    "### Cluster reads number\n",
    "Cluster_reads_count = order_DF.groupby([\"read_name\"])[\"Cluster\"].first()\n",
    "Cluster_reads_count = pd.DataFrame(Cluster_reads_count).reset_index()\n",
    "Cluster_reads_count.columns = [\"Read_name\",\"Cluster\"]\n",
    "summary_count = Cluster_reads_count.Cluster.value_counts()\n",
    "Cluster_reads_count = pd.DataFrame({\"Cluster\":summary_count.index,\n",
    "                                   \"Count\":summary_count})\n",
    "Cluster_reads_count.loc[:,\"y\"] = 0\n",
    "for n, rowvalue in Cluster_reads_count.iterrows():\n",
    "    cluster = rowvalue[\"Cluster\"]\n",
    "    cluster_count = rowvalue[\"Count\"]\n",
    "    ylist = set( order_DF.loc[order_DF.Cluster == cluster, \"read_order\"].to_list() )\n",
    "    yvalue = sum( ylist )/len(ylist)\n",
    "    Cluster_reads_count.loc[n][\"y\"] = yvalue\n",
    "    plt.text(region[2], yvalue,  \"Cluster %d\\n%d\"%(cluster, cluster_count) )\n",
    "\n",
    "ax2 = plt.subplot(5,1,3, sharex=ax1)\n",
    "# Calculate Bin cluster read count and bin readcounts\n",
    "cluster_readcount_df = ClusterReads(order_DF)\n",
    "#print(cluster_readcount_df)\n",
    "All_reads_num = Cluster_reads_count.Count.sum() \n",
    "BinDensity_df = FragmentDensity(order_DF,All_reads_num)\n",
    "BinDensity_df.loc[:,\"BinPos\"] = BinDensity_df.loc[:,\"bin\"].values * binsize\n",
    "# Plot Density\n",
    "groupDF = BinDensity_df.groupby(\"Cluster\")\n",
    "Clist = []\n",
    "for cluster, gdf in groupDF:\n",
    "    cluster = int(cluster)\n",
    "    x, y = gdf.BinPos.values, gdf.relative_ratio.values\n",
    "    x, y = smoothline(x, y, 50)\n",
    "    plt.plot(x, y, c=colors[cluster], linewidth=1)\n",
    "    Clist.append(cluster)\n",
    "plt.legend(Clist, loc='upper left')\n",
    "plt.xlim([region[1], region[2]])\n",
    "plt.yscale('log')\n",
    "plt.yticks([0.001, 0.1],[\"0.001\", \"0.1\"])\n",
    "plt.ylabel(\"Fragment\\nrelative freq\")\n",
    "plt.xticks([])\n",
    "\n",
    "\n",
    "\n",
    "# CTCF contact intensity\n",
    "CTCF_DF = order_DF.loc[order_DF.binfind==1, :]\n",
    "CTCF_DF.loc[:, \"binPos\"] =  CTCF_DF.bin.values * binsize\n",
    "CTCF_DF = CTCF_DF.astype({\"Cluster\": \"str\"})\n",
    "CTCFbins = list( set( CTCF_DF.binPos.to_list() ) )\n",
    "CTCFbins = sorted(CTCFbins)\n",
    "\n",
    "### CTCF relative count plot\n",
    "ctcf_count = CTCF_DF.groupby([\"Cluster\", \"binPos\"], as_index=False)[\"read_name\"].count()\n",
    "ctcf_count.columns = [\"Cluster\", \"binPos\", \"fcount\"]\n",
    "\n",
    "ax3 = plt.subplot(5,1,4, sharex=ax1)\n",
    "Clist = []\n",
    "for cluster, df in ctcf_count.groupby(\"Cluster\"):\n",
    "    x, y = df.binPos.values, df.fcount.values\n",
    "    ## calculate relative ctcf percentages = ctcfbins / cluter read numbers\n",
    "    y = y / summary_count[int(cluster)]\n",
    "    plt.plot(x,y, linewidth=1.5, color = colors[int(cluster)], alpha=0.5, marker='.', linestyle='-')\n",
    "    Clist.append(cluster)\n",
    "plt.legend(Clist, loc='best')\n",
    "plt.ylabel(\"CTCF\\n relative freq\")\n",
    "plt.yscale('log')\n",
    "plt.yticks([0.001, 0.1],[\"0.001\", \"0.1\"])\n",
    "plt.xticks([])\n",
    "\n",
    "\n",
    "### CTCF pair contacts\n",
    "import itertools\n",
    "Pairs_count = {\"Cluster\":[],\n",
    "               \"read_name\":[],\n",
    "              \"ctcf1\":[],\n",
    "              \"ctcf2\":[]}\n",
    "for c1, c2 in itertools.combinations(CTCFbins, 2):\n",
    "    #print (c1, c2)\n",
    "    for (cluster, read_name), gdf in CTCF_DF.groupby([\"Cluster\", \"read_name\"]):\n",
    "        binlist = gdf.binPos.to_list()\n",
    "        if c1 in binlist and c2 in binlist:\n",
    "            Pairs_count[\"Cluster\"].append(cluster)\n",
    "            Pairs_count[\"read_name\"].append(read_name)\n",
    "            Pairs_count[\"ctcf1\"].append(c1)\n",
    "            Pairs_count[\"ctcf2\"].append(c2)\n",
    "Pairs_count = pd.DataFrame(Pairs_count)\n",
    "Pairs_summary = Pairs_count.groupby([\"ctcf1\", \"ctcf2\", \"Cluster\"])[\"read_name\"].count().reset_index()\n",
    "Pairs_summary.columns = [\"ctcf1\",\"ctcf2\",\"Cluster\",\"Count\"]\n",
    "\n",
    "ax4 = plt.subplot(5,1,5, sharex=ax1)\n",
    "for n, rowvalue in Pairs_summary.iterrows():\n",
    "    cluster = rowvalue[\"Cluster\"]\n",
    "    \n",
    "    i, j  = rowvalue[\"ctcf1\"], rowvalue[\"ctcf2\"]\n",
    "    middle = (i+j)/2\n",
    "    widthsize = j-i\n",
    "    weight =  rowvalue[\"Count\"]\n",
    "    weight = weight/summary_count[int(cluster)]  # relative height\n",
    "    \n",
    "    X = np.linspace(i,j, 50)\n",
    "    #Y = ( - (X-middle)**2 + ( (widthsize/2)**2 ) )*int(cluster)\n",
    "    Y = ( - (X-middle)**2 + ( (widthsize/2)**2 ) ) / (widthsize/2)**2 * float(weight)\n",
    "    plt.plot(X,Y, linewidth=1.5, color = colors[int(cluster)], alpha=0.5)\n",
    "plt.ylabel(\"CTCF pair-contact\\nrelative freq\")\n",
    "\n",
    "\n",
    "### xticks and save \n",
    "# xlim, xticks\n",
    "plt.xlim([region[1], region[2]])\n",
    "# xticks\n",
    "Xtick = list( np.linspace(region[1], region[2], 5 ,endpoint=True) )\n",
    "Xtick_label = [ \"%.3f\"%(i/10**6) for i in Xtick ]\n",
    "plt.xticks( Xtick, Xtick_label)\n",
    "plt.xlabel(\"%s Mb\"%region[0] )\n",
    "Figname = \"Doublebel\"\n",
    "\n",
    "Exportdir = \"%s_%d_%dGM12878\"%(region[0], region[1], region[2])\n",
    "os.system(\"mkdir -p %s\"%Exportdir)\n",
    "plt.savefig(\"%s/TAD_clusters_%s:%d-%d.pdf\"%(Exportdir,  region[0], region[1], region[2]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8e7655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Random sample reads\n",
    "'''\n",
    "1. Sample reads\n",
    "2. Get reads with more than ? fragments\n",
    "'''\n",
    "\n",
    "def Reorder(df):\n",
    "    '''\n",
    "    Reorder read fragment by the first frag\n",
    "    '''\n",
    "    re_df = df.groupby([\"Cluster\",\"read_name\"])[\"start\"].min().reset_index()\n",
    "    re_df = re_df.sort_values(by=[\"Cluster\",\"start\"], ignore_index=True).reset_index().set_index(\"read_name\")\n",
    "    df.loc[:, \"FragY\"] = re_df.loc[df.read_name.to_list(), \"index\"].values\n",
    "    return(df)\n",
    "\n",
    "from sklearn.utils import resample  \n",
    "\n",
    "Samplenum = 50\n",
    "Samplerate = 0.50\n",
    "Fragthred = 6\n",
    "Pnum = order_DF.Fragnum >= Fragthred\n",
    "SampleList = [] # Get sample reads\n",
    "for cluster, gdf in order_DF.loc[Pnum].groupby([\"Cluster\"], as_index=False):\n",
    "    Creads = list( set( gdf.read_name.to_list()  ) )  # cluter reads\n",
    "    Samplenum = int( Samplerate * len(Creads) )\n",
    "    Sreads = resample(Creads, n_samples=Samplenum,replace=1) # sample reads\n",
    "    SampleList.append( gdf.loc[gdf.read_name.isin(Sreads) , :] )\n",
    "Sampledf = pd.concat(SampleList) # Sample df\n",
    "Sampledf = Reorder(Sampledf)    \n",
    " \n",
    "\n",
    "## Cluster read numbers\n",
    "Cluster_reads_count = order_DF.groupby([\"read_name\"])[\"Cluster\"].first()\n",
    "Cluster_reads_count = pd.DataFrame(Cluster_reads_count).reset_index()\n",
    "Cluster_reads_count.columns = [\"Read_name\",\"Cluster\"]\n",
    "summary_count = Cluster_reads_count.Cluster.value_counts()\n",
    "Cluster_reads_count = pd.DataFrame({\"Cluster\":summary_count.index,\n",
    "                                   \"Count\":summary_count})    \n",
    "    \n",
    "# Cluster Figure 1\n",
    "#plt.figure(figsize=(8,10) )\n",
    "FigRow, FigCol, N = 7, 1, 1\n",
    "plt.subplots(FigRow, FigCol, figsize=(6,12), sharex=True)\n",
    "Axlist = []\n",
    "subsample = 1\n",
    "for cluster, cgdf in Sampledf.groupby(\"Cluster\"):\n",
    "    Axlist.append( plt.subplot(FigRow, FigCol, N)  )\n",
    "    N += 1\n",
    "    for read_name, gdf in cgdf.groupby(\"read_name\"):\n",
    "        yn = gdf[\"FragY\"].values[0]\n",
    "        if yn % subsample == 0:\n",
    "            # grey line\n",
    "            read_xs = [ gdf[\"bin\"].min()*binsize, gdf[\"bin\"].max()*binsize]\n",
    "            ys = [yn, yn ]\n",
    "            #plt.plot(read_xs, ys, c=\"lightgrey\", linewidth=0.2) # fragment link lines\n",
    "            # Fragments\n",
    "            for indx, rowvalue in gdf.iterrows():\n",
    "                yn = rowvalue[\"FragY\"]\n",
    "                sc = colors[cluster]\n",
    "                if rowvalue[\"binfind\"] == 1:\n",
    "                    sc = \"k\" # ctct bin fragments black\n",
    "                start = rowvalue[\"bin\"]*binsize - 0.5*binsize\n",
    "                end = rowvalue[\"bin\"]*binsize + 0.5*binsize\n",
    "                xs = [ start, end ]\n",
    "                ys = [yn, yn ]\n",
    "                # Fragment\n",
    "                plt.plot(xs, ys, c=sc, linewidth=1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    readnum = summary_count[cluster]\n",
    "    plt.ylabel( \"Cluster %d\\nN=%d\"%(cluster, readnum) )\n",
    "\n",
    "    \n",
    "Axlist.append( plt.subplot(FigRow, FigCol, N)  )\n",
    "N += 1\n",
    "# Calculate Bin cluster read count and bin readcounts\n",
    "cluster_readcount_df = ClusterReads(order_DF)\n",
    "#print(cluster_readcount_df)\n",
    "All_reads_num = Cluster_reads_count.Count.sum() \n",
    "BinDensity_df = FragmentDensity(order_DF,All_reads_num)\n",
    "BinDensity_df.loc[:,\"BinPos\"] = BinDensity_df.loc[:,\"bin\"].values * binsize\n",
    "# Plot Density\n",
    "groupDF = BinDensity_df.groupby(\"Cluster\")\n",
    "Clist = []\n",
    "for cluster, gdf in groupDF:\n",
    "    cluster = int(cluster)\n",
    "    x, y = gdf.BinPos.values, gdf.relative_ratio.values\n",
    "    x, y = smoothline(x, y, 100)\n",
    "    plt.plot(x, y, c=colors[cluster], linewidth=1)\n",
    "    Clist.append(cluster)\n",
    "plt.legend(Clist, loc='upper left')\n",
    "#plt.xlim([region[1], region[2]])\n",
    "#plt.yscale('log')\n",
    "#plt.yticks([0.001, 0.1],[\"0.001\", \"0.1\"])\n",
    "plt.ylabel(\"Fragment\\nrelative freq\")\n",
    "plt.xticks([])\n",
    "\n",
    "\n",
    "\n",
    "# CTCF contact intensity\n",
    "CTCF_DF = order_DF.loc[order_DF.binfind==1, :]\n",
    "CTCF_DF.loc[:, \"binPos\"] =  CTCF_DF.bin.values * binsize\n",
    "CTCF_DF = CTCF_DF.astype({\"Cluster\": \"str\"})\n",
    "CTCFbins = list( set( CTCF_DF.binPos.to_list() ) )\n",
    "CTCFbins = sorted(CTCFbins)\n",
    "### CTCF relative count plot\n",
    "ctcf_count = CTCF_DF.groupby([\"Cluster\", \"binPos\"], as_index=False)[\"read_name\"].count()\n",
    "ctcf_count.columns = [\"Cluster\", \"binPos\", \"fcount\"]\n",
    "### CTCF pair contacts\n",
    "import itertools\n",
    "Pairs_count = {\"Cluster\":[],\n",
    "               \"read_name\":[],\n",
    "              \"ctcf1\":[],\n",
    "              \"ctcf2\":[]}\n",
    "for c1, c2 in itertools.combinations(CTCFbins, 2):\n",
    "    #print (c1, c2)\n",
    "    for (cluster, read_name), gdf in CTCF_DF.groupby([\"Cluster\", \"read_name\"]):\n",
    "        binlist = gdf.binPos.to_list()\n",
    "        if c1 in binlist and c2 in binlist:\n",
    "            Pairs_count[\"Cluster\"].append(cluster)\n",
    "            Pairs_count[\"read_name\"].append(read_name)\n",
    "            Pairs_count[\"ctcf1\"].append(c1)\n",
    "            Pairs_count[\"ctcf2\"].append(c2)\n",
    "Pairs_count = pd.DataFrame(Pairs_count)\n",
    "Pairs_summary = Pairs_count.groupby([\"ctcf1\", \"ctcf2\", \"Cluster\"])[\"read_name\"].count().reset_index()\n",
    "Pairs_summary.columns = [\"ctcf1\",\"ctcf2\",\"Cluster\",\"Count\"]\n",
    "## pairs filter \n",
    "'''\n",
    "ctcf pair count >= 3\n",
    "ctcf pair region >= 25kb\n",
    "'''\n",
    "Pairs_summary = Pairs_summary.loc[Pairs_summary.Count >= 3, :]\n",
    "Psize = abs( Pairs_summary.ctcf1.values - Pairs_summary.ctcf2.values ) >= 25000\n",
    "Pairs_summary = Pairs_summary.loc[Psize, :]\n",
    "\n",
    "## CTCF freq figure\n",
    "Axlist.append( plt.subplot(FigRow, FigCol, N)  )\n",
    "N += 1\n",
    "Clist = []\n",
    "for cluster, df in ctcf_count.groupby(\"Cluster\"):\n",
    "    x, y = df.binPos.values, df.fcount.values\n",
    "    ## calculate relative ctcf percentages = ctcfbins / cluter read numbers\n",
    "    y = y / summary_count[int(cluster)]\n",
    "    plt.plot(x,y, linewidth=1.5, color = colors[int(cluster)], alpha=0.5, marker='.', linestyle='-')\n",
    "    Clist.append(cluster)\n",
    "plt.legend(Clist, loc='best')\n",
    "plt.ylabel(\"CTCF\\n relative freq\")\n",
    "#plt.yscale('log')\n",
    "#plt.yticks([0.001, 0.1],[\"0.001\", \"0.1\"])\n",
    "plt.xticks([])\n",
    "#plt.xlim([region[1], region[2]])\n",
    "\n",
    "\n",
    "## CTCF pairs contact\n",
    "Axlist.append( plt.subplot(FigRow, FigCol, N)  )\n",
    "N += 1\n",
    "for n, rowvalue in Pairs_summary.iterrows():\n",
    "    cluster = rowvalue[\"Cluster\"]\n",
    "    \n",
    "    i, j  = rowvalue[\"ctcf1\"], rowvalue[\"ctcf2\"]\n",
    "    middle = (i+j)/2\n",
    "    widthsize = j-i\n",
    "    weight =  rowvalue[\"Count\"]\n",
    "    weight = weight/summary_count[int(cluster)]  # relative height\n",
    "    \n",
    "    X = np.linspace(i,j, 50)\n",
    "    #Y = ( - (X-middle)**2 + ( (widthsize/2)**2 ) )*int(cluster)\n",
    "    Y = ( - (X-middle)**2 + ( (widthsize/2)**2 ) ) / (widthsize/2)**2 * float(weight)\n",
    "    plt.plot(X,Y, linewidth=1.5, color = colors[int(cluster)], alpha=0.5)\n",
    "plt.ylabel(\"CTCF pair-contact\\nrelative freq\")\n",
    "#plt.yscale('log')\n",
    "\n",
    "## CTCF motif orientations\n",
    "# loading ctcf motif information\n",
    "ctcf_motif_file=\"GM12878_CTCF_IDR_peaks.motif.annotation.bed\"\n",
    "motifDF = Loadingbed(ctcf_motif_file, region, binsize, [\"chrom\", \"start\", \"end\",\"strand\"], [0,1,2,5] )\n",
    "motifDF = motifDF.sort_values(by=\"pos\", ignore_index=True)\n",
    "\n",
    "Axlist.append( plt.subplot(FigRow, FigCol, N)  )\n",
    "N += 1\n",
    "y = 1\n",
    "for n, rowvalue in motifDF.iterrows():\n",
    "    x, strand_val = rowvalue[\"pos\"], rowvalue[\"strand\"]\n",
    "    x_u = 1\n",
    "    if strand_val == \"-\":\n",
    "        x_u = -1\n",
    "    plt.quiver(x,y,x_u,0, scale=40) # arrow, + , -\n",
    "    y = y*(-1) # two lines\n",
    "plt.yticks([])\n",
    "plt.ylim([-2, 2])\n",
    "plt.xticks([])\n",
    "plt.ylabel(\"CTCF motif\\norientation\")\n",
    "\n",
    "flank = 0.05*10**6\n",
    "plt.xlim([region[1]-flank, region[2]+flank])\n",
    "# xticks\n",
    "Xtick = list( np.linspace(region[1], region[2], 5 ,endpoint=True) )\n",
    "Xtick_label = [ \"%.3f\"%(i/10**6) for i in Xtick ]\n",
    "plt.xticks( Xtick, Xtick_label)\n",
    "plt.xlabel(\"%s Mb\"%region[0] )\n",
    "#plt.show()\n",
    "\n",
    "Exportdir = \"%s_%d_%dGM12878\"%(region[0], region[1], region[2])\n",
    "os.system(\"mkdir -p %s\"%Exportdir)\n",
    "plt.savefig(\"%s/TAD_clusters_%s:%d-%d_v1.pdf\"%(Exportdir,  region[0], region[1], region[2]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06ad6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### CTCF pair contacts\n",
    "motifdict = {}\n",
    "for n, rowdf in motifDF.iterrows():\n",
    "    binID, strand = int(rowdf[\"bin\"]*binsize), rowdf[\"strand\"]\n",
    "    motifdict[binID] = strand\n",
    "## ctcf combination pattern()\n",
    "ctcfPattern ={\"--\":\"tendom\",\n",
    "              \"++\":\"tendom\",\n",
    "              \"+-\":\"convergen\",\n",
    "              \"-+\":\"divergen\"}\n",
    "import itertools\n",
    "Pairs_count = {\"Cluster\":[],\n",
    "               \"read_name\":[],\n",
    "              \"ctcf1\":[],\n",
    "              \"ctcf2\":[],\n",
    "               \"ctcf_combin\":[],\n",
    "              \"ctcf_pattern\":[]}\n",
    "for c1, c2 in itertools.combinations(CTCFbins, 2):\n",
    "    #print (c1, c2)\n",
    "    for (cluster, read_name), gdf in CTCF_DF.groupby([\"Cluster\", \"read_name\"]):\n",
    "        binlist = gdf.binPos.to_list()\n",
    "        if c1 in binlist and c2 in binlist:\n",
    "            Pairs_count[\"Cluster\"].append(cluster)\n",
    "            Pairs_count[\"read_name\"].append(read_name)\n",
    "            Pairs_count[\"ctcf1\"].append(c1)\n",
    "            Pairs_count[\"ctcf2\"].append(c2)\n",
    "            # orientation\n",
    "            try:\n",
    "                ctcf_combin = motifdict[c1] + motifdict[c2]\n",
    "                ctcf_p = ctcfPattern[ctcf_combin]\n",
    "            except:\n",
    "                ctcf_combin = \"Unknown\"\n",
    "                ctcf_p = \"Unknown\"\n",
    "            Pairs_count[\"ctcf_combin\"].append(ctcf_combin)\n",
    "            Pairs_count[\"ctcf_pattern\"].append(ctcf_p)\n",
    "            \n",
    "Pairs_count = pd.DataFrame(Pairs_count)\n",
    "## filter 20kb ctcf pairs\n",
    "Psize = abs( Pairs_count.ctcf1.values - Pairs_count.ctcf2.values ) >= 20000\n",
    "Pairs_count = Pairs_count.loc[Psize, :]\n",
    "Pairs_count  = Pairs_count.loc[Pairs_count.ctcf_combin != \"Unknown\", :]\n",
    "\n",
    "PatternSummary = {\"Cluster\":[], \n",
    "                  \"AllelPattern\":[]}\n",
    "for read_name, df in Pairs_count.groupby(\"read_name\"):\n",
    "    Cluster = df[\"Cluster\"].iloc[0]\n",
    "    patterns = df[\"ctcf_pattern\"].to_list()\n",
    "    if len(patterns) >= 2:\n",
    "        p1 = \"multi:\"\n",
    "    else:\n",
    "        p1 = \"single:\"\n",
    "    set_pattern = list( set(patterns) )\n",
    "    if len(set_pattern) >= 2:\n",
    "        p2 = \"+\".join(set_pattern)\n",
    "    else:\n",
    "        p2 = set_pattern[0]\n",
    "    AllelPattern = p1+p2\n",
    "    PatternSummary[\"Cluster\"].append(Cluster)\n",
    "    PatternSummary[\"AllelPattern\"].append(AllelPattern )\n",
    "PatternSummary = pd.DataFrame(PatternSummary)    \n",
    "PatternSummary  = PatternSummary.sort_values([\"Cluster\", \"AllelPattern\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d080ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "PatternSummary  = PatternSummary.sort_values([\"AllelPattern\"])\n",
    "f, ax = plt.subplots(figsize=(6, 6))\n",
    "sns.despine(f)\n",
    "sns.histplot(\n",
    "    PatternSummary,\n",
    "    x=\"Cluster\", hue=\"AllelPattern\",\n",
    "    multiple=\"fill\",\n",
    "    palette=\"Paired\",\n",
    "    edgecolor=\".3\",\n",
    "    linewidth=.5)\n",
    "plt.ylabel(\"Percentage\")\n",
    "plt.savefig(\"%s/TAD_clusters_%s:%d-%d_CTCF_pattern.pdf\"%(Exportdir,  region[0], region[1], region[2]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af8cfa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export cluster vdFragments\n",
    "Generate_Contactpy = \"Generate_Contact_juiceMatrix_vdF.py\"\n",
    "Generate_coolsh = \"Generate_cool.sh\"\n",
    "export_cols =  [\"read_name\",\"read_start\",\"read_end\",\"strand\",\"chrom\",\"start\",\"end\",\"MapQual\",\"LRvdF_pfix\"]\n",
    "# clear all vdFragment files coolfiles in the Exoportdir\n",
    "os.system(\"mkdir -p %s\"%Exportdir)\n",
    "#os.system( \"rm %s/*._juice_matrix.txt\"%(Exportdir))\n",
    "#os.system( \"rm %s/*.cool\"%(Exportdir))\n",
    "#os.system( \"rm %s/*.mcool\"%(Exportdir))\n",
    "\n",
    "## Generate juicematrix\n",
    "for cluster, df in  order_DF.groupby(\"Cluster\"):\n",
    "    inputfile = Exportdir + \"/\" +\"vdFragment.csv\"\n",
    "    juice_matrix = Exportdir + \"/\" + \"C%d_juice_matrix.txt\"%(cluster)\n",
    "    # export vdFragment.csv \n",
    "    clt_VDF_DF = df.loc[:, export_cols]\n",
    "    clt_VDF_DF.to_csv(inputfile, sep=\",\", header=True, index=False )\n",
    "    # python <Generate_Contactpy> <inputfile> <juice_matrix> \n",
    "    os.system( \"python %s %s %s\"%(Generate_Contactpy, inputfile, juice_matrix) )  \n",
    "    os.system( \"bash %s %s %s\"%(Generate_coolsh, Exportdir, juice_matrix) ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eece07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genome distance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
